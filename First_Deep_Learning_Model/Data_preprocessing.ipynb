{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb17195-a0cf-47ff-82fc-106b16607e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\buw-\n",
      "[nltk_data]     ki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data Preprocessing and Feature Engineering for Customer Satisfaction Prediction\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d878c584-0f3c-4896-8b23-e54f056a4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and exploring data...\n",
      "Dataset shape: (13594, 11)\n",
      "\n",
      "Target variable distribution:\n",
      "ReviewRating\n",
      "1    7082\n",
      "2     850\n",
      "3     644\n",
      "4    1099\n",
      "5    3919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class imbalance ratio: 11.00\n",
      "\n",
      "Preprocessing features...\n",
      "Cleaning text data...\n",
      "Extracting text features...\n",
      "\n",
      "Preparing text sequences for deep learning...\n",
      "Text sequences shape: (13594, 100)\n",
      "Vocabulary size: 27318\n",
      "\n",
      "Splitting data into train (70.0%), validation (10.0%), and test (20.0%) sets...\n",
      "Train set size: 9515\n",
      "Validation set size: 1360\n",
      "Test set size: 2719\n",
      "\n",
      "Handling class imbalance using combined strategy...\n",
      "Original distribution: [4957  595  451  769 2743]\n",
      "Balanced distribution: [4957   40 4957 4957 4957]\n",
      "\n",
      "==================================================\n",
      "Data preprocessing completed successfully!\n",
      "==================================================\n",
      "Numerical features shape: (9515, 14)\n",
      "Text features shape: (9515, 100)\n",
      "Balanced training set size: 19868\n",
      "Class weights: {0: np.float64(0.3839015533588864), 1: np.float64(3.1983193277310926), 2: np.float64(4.219512195121951), 3: np.float64(2.4746423927178154), 4: np.float64(0.6937659496901203)}\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove URLs, email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Remove special characters and digits, keep only alphabets and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
    "                  if token not in self.stop_words and len(token) > 2]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def extract_text_features(self, text):\n",
    "        \"\"\"Extract additional features from text\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return {\n",
    "                'text_length': 0,\n",
    "                'word_count': 0,\n",
    "                'avg_word_length': 0,\n",
    "                'exclamation_count': 0,\n",
    "                'question_count': 0,\n",
    "                'upper_case_ratio': 0\n",
    "            }\n",
    "\n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "\n",
    "        return {\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(words),\n",
    "            'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "            'exclamation_count': text.count('!'),\n",
    "            'question_count': text.count('?'),\n",
    "            'upper_case_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        }\n",
    "\n",
    "    def load_and_explore_data(self, filepath):\n",
    "        \"\"\"Load data and perform initial exploration\"\"\"\n",
    "        print(\"Loading and exploring data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"\\nTarget variable distribution:\")\n",
    "        print(df['ReviewRating'].value_counts().sort_index())\n",
    "\n",
    "        # Calculate class imbalance ratio\n",
    "        rating_counts = df['ReviewRating'].value_counts()\n",
    "        imbalance_ratio = rating_counts.max() / rating_counts.min()\n",
    "        print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "        # Visualize target distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        df['ReviewRating'].value_counts().sort_index().plot(kind='bar')\n",
    "        plt.title('Review Rating Distribution')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        df['ReviewRating'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Review Rating Distribution (%)')\n",
    "        plt.ylabel('')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('charts/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def preprocess_features(self, df):\n",
    "        \"\"\"Preprocess all features\"\"\"\n",
    "        print(\"\\nPreprocessing features...\")\n",
    "\n",
    "        # Clean text features\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['ReviewText_clean'] = df['ReviewText'].apply(self.clean_text)\n",
    "        df['ReviewTitle_clean'] = df['ReviewTitle'].apply(self.clean_text)\n",
    "\n",
    "        # Combine text features\n",
    "        df['combined_text'] = df['ReviewText_clean'] + ' ' + df['ReviewTitle_clean']\n",
    "\n",
    "        # Extract text features\n",
    "        print(\"Extracting text features...\")\n",
    "        text_features = df['ReviewText'].apply(self.extract_text_features)\n",
    "        text_features_df = pd.DataFrame(text_features.tolist())\n",
    "\n",
    "        title_features = df['ReviewTitle'].apply(self.extract_text_features)\n",
    "        title_features_df = pd.DataFrame(title_features.tolist())\n",
    "        title_features_df.columns = ['title_' + col for col in title_features_df.columns]\n",
    "\n",
    "        # Combine all features\n",
    "        df = pd.concat([df, text_features_df, title_features_df], axis=1)\n",
    "\n",
    "        # Encode categorical features\n",
    "        categorical_features = ['UserCountry']\n",
    "        for feature in categorical_features:\n",
    "            if feature in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[f'{feature}_encoded'] = le.fit_transform(df[feature].astype(str))\n",
    "                self.label_encoders[feature] = le\n",
    "\n",
    "        # Select final features for modeling\n",
    "        feature_columns = [\n",
    "            'ReviewCount', 'UserCountry_encoded',\n",
    "            'text_length', 'word_count', 'avg_word_length',\n",
    "            'exclamation_count', 'question_count', 'upper_case_ratio',\n",
    "            'title_text_length', 'title_word_count', 'title_avg_word_length',\n",
    "            'title_exclamation_count', 'title_question_count', 'title_upper_case_ratio'\n",
    "        ]\n",
    "\n",
    "        # Handle missing values\n",
    "        for col in feature_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df, feature_columns\n",
    "\n",
    "    def create_balanced_dataset(self, X, y, strategy='combined'):\n",
    "        \"\"\"Handle class imbalance using various techniques\"\"\"\n",
    "        print(f\"\\nHandling class imbalance using {strategy} strategy...\")\n",
    "\n",
    "        if strategy == 'smote':\n",
    "            # Use SMOTE for oversampling\n",
    "            smote = SMOTE(random_state=42, k_neighbors=2)  # Reduced k_neighbors due to small dataset\n",
    "            X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "        elif strategy == 'undersampling':\n",
    "            # Use random undersampling\n",
    "            undersampler = RandomUnderSampler(random_state=42)\n",
    "            X_balanced, y_balanced = undersampler.fit_resample(X, y)\n",
    "\n",
    "        elif strategy == 'combined':\n",
    "            # Combined approach: first oversample minority classes, then undersample majority\n",
    "            # Step 1: Oversample very minority classes\n",
    "            smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "            X_temp, y_temp = smote.fit_resample(X, y)\n",
    "\n",
    "            # Step 2: Undersample majority class\n",
    "            undersampler = RandomUnderSampler(random_state=42,\n",
    "                                              sampling_strategy={1: 40})  # Reduce class 1 to 40 samples\n",
    "            X_balanced, y_balanced = undersampler.fit_resample(X_temp, y_temp)\n",
    "\n",
    "        else:\n",
    "            X_balanced, y_balanced = X, y\n",
    "\n",
    "        print(f\"Original distribution: {np.bincount(y)}\")\n",
    "        print(f\"Balanced distribution: {np.bincount(y_balanced)}\")\n",
    "\n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "    def prepare_text_sequences(self, df, max_features=5000, max_len=100):\n",
    "        \"\"\"Prepare text data for deep learning models\"\"\"\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "        print(\"\\nPreparing text sequences for deep learning...\")\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(df['combined_text'])\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "\n",
    "        # Pad sequences\n",
    "        X_text = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "        print(f\"Text sequences shape: {X_text.shape}\")\n",
    "        print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "\n",
    "        return X_text, tokenizer\n",
    "\n",
    "    def split_data(self, X_numerical, X_text, y, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "        print(\n",
    "            f\"\\nSplitting data into train ({1 - test_size - val_size:.1%}), validation ({val_size:.1%}), and test ({test_size:.1%}) sets...\")\n",
    "\n",
    "        # First split: separate test set\n",
    "        X_num_temp, X_num_test, X_text_temp, X_text_test, y_temp, y_test = train_test_split(\n",
    "            X_numerical, X_text, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust validation size\n",
    "        X_num_train, X_num_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "            X_num_temp, X_text_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "        )\n",
    "\n",
    "        print(f\"Train set size: {len(X_num_train)}\")\n",
    "        print(f\"Validation set size: {len(X_num_val)}\")\n",
    "        print(f\"Test set size: {len(X_num_test)}\")\n",
    "\n",
    "        # Scale numerical features\n",
    "        X_num_train_scaled = self.scaler.fit_transform(X_num_train)\n",
    "        X_num_val_scaled = self.scaler.transform(X_num_val)\n",
    "        X_num_test_scaled = self.scaler.transform(X_num_test)\n",
    "\n",
    "        return (X_num_train_scaled, X_num_val_scaled, X_num_test_scaled,\n",
    "                X_text_train, X_text_val, X_text_test,\n",
    "                y_train, y_val, y_test)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create output directories\n",
    "    import os\n",
    "    os.makedirs('charts', exist_ok=True)\n",
    "\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DataPreprocessor()\n",
    "\n",
    "    # Load and explore data\n",
    "    df = preprocessor.load_and_explore_data('data/temu_reviews_cleaned.csv')\n",
    "\n",
    "    # Preprocess features\n",
    "    df_processed, feature_columns = preprocessor.preprocess_features(df)\n",
    "\n",
    "    # Prepare numerical features\n",
    "    X_numerical = df_processed[feature_columns].values\n",
    "\n",
    "    # Prepare text sequences\n",
    "    X_text, tokenizer = preprocessor.prepare_text_sequences(df_processed)\n",
    "\n",
    "    # Prepare target variable\n",
    "    y = df_processed['ReviewRating'].values - 1  # Convert to 0-4 for neural networks\n",
    "\n",
    "    # Split data\n",
    "    (X_num_train, X_num_val, X_num_test,\n",
    "     X_text_train, X_text_val, X_text_test,\n",
    "     y_train, y_val, y_test) = preprocessor.split_data(X_numerical, X_text, y)\n",
    "\n",
    "    # Handle class imbalance for training data only\n",
    "    # Combine numerical and text features for balancing\n",
    "    X_train_combined = np.concatenate([X_num_train, X_text_train], axis=1)\n",
    "    X_train_balanced, y_train_balanced = preprocessor.create_balanced_dataset(\n",
    "        X_train_combined, y_train, strategy='combined'\n",
    "    )\n",
    "\n",
    "    # Split back into numerical and text features\n",
    "    num_features = X_num_train.shape[1]\n",
    "    X_num_train_balanced = X_train_balanced[:, :num_features]\n",
    "    X_text_train_balanced = X_train_balanced[:, num_features:].astype(int)\n",
    "\n",
    "    # Calculate class weights for models\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Save preprocessed data\n",
    "    np.savez('data/preprocessed_data.npz',\n",
    "             X_num_train=X_num_train, X_num_val=X_num_val, X_num_test=X_num_test,\n",
    "             X_text_train=X_text_train, X_text_val=X_text_val, X_text_test=X_text_test,\n",
    "             X_num_train_balanced=X_num_train_balanced,\n",
    "             X_text_train_balanced=X_text_train_balanced,\n",
    "             y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "             y_train_balanced=y_train_balanced)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'feature_columns': feature_columns,\n",
    "        'vocab_size': len(tokenizer.word_index) + 1,\n",
    "        'max_sequence_length': X_text.shape[1],\n",
    "        'num_classes': len(np.unique(y)),\n",
    "        'class_weights': class_weight_dict,\n",
    "        'class_names': ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']\n",
    "    }\n",
    "\n",
    "    import pickle\n",
    "    with open('data/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    with open('data/tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Data preprocessing completed successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Numerical features shape: {X_num_train.shape}\")\n",
    "    print(f\"Text features shape: {X_text_train.shape}\")\n",
    "    print(f\"Balanced training set size: {len(X_num_train_balanced)}\")\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137338-b4fc-465e-bf86-5023538bd157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
